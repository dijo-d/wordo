
# ğŸ¦™ WordO â€“ The Word Generator

**WordO** is a fine-tuned version of Metaâ€™s [LLaMA 3](https://huggingface.co/meta-llama), specifically trained to generate **precise English words** in response to given statements or definitions. It is designed to assist in tasks like vocabulary building, word retrieval, educational exercises, and language enrichment.

- ğŸ“¦ **Model on Hugging Face**: [dijodaiju/llama-3-wordo-wordgen](https://huggingface.co/dijodaiju/llama-3-wordo-wordgen)  
- ğŸ§¾ **Notebook on GitHub**: [dijo-d/wordo](https://github.com/dijo-d/wordo)

---

## ğŸ§  Model Objective

WordO focuses on **accurately predicting real English words** based on:

- ğŸ”¹ Word definitions
- ğŸ”¹ Example usage
- ğŸ”¹ Descriptive prompts

Trained using a custom dataset that includes **words, meanings, and usage examples**, the model is optimized for understanding context and suggesting appropriate, real-world vocabulary.

---

## âœ¨ Key Features

- ğŸ¯ **Precision-first word generation**
- ğŸ“˜ Trained on a curated English vocabulary dataset
- ğŸ§  Based on `Meta-Llama-3-8B-Instruct`
- ğŸ“š Ideal for educational use, vocabulary training, or linguistic projects

---

## ğŸš€ Quick Start (Inference)

You can load and use the model directly from Hugging Face:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "dijodaiju/llama-3-wordo-wordgen"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)

prompt = "A word meaning 'the fear of being forgotten'"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=5)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

---

## ğŸ—‚ï¸ Repository Contents

This GitHub repository contains:

- ğŸ““ `wordgen_demo.ipynb` â€“ A simple notebook demonstrating model usage
- ğŸ“ Example prompts and outputs
- ğŸ“ Notes on fine-tuning, data format, and limitations

> ğŸ”§ The model is not yet deployed as an API or UI â€“ currently accessible via notebook and Hugging Face.

---

## ğŸ“Š Training Dataset

The model was fine-tuned on a custom English vocabulary dataset with the following format:

```json
{
  "word": "ephemeral",
  "meaning": "lasting for a very short time",
  "example": "The beauty of the cherry blossoms is ephemeral."
}
```

This format was converted into structured prompts to guide the LLaMA 3 model toward accurate word prediction.

---

## ğŸ§ª Example Prompt

```
Definition: "the practice of looking inward to examine one's own thoughts and feelings"
Example usage: "Through meditation, she developed a strong sense of ________."
â†’ introspection
```

---

## ğŸ“ Future Plans

- [ ] FastAPI/Gradio interface for interactive use
- [ ] Hugging Face Space deployment
- [ ] Multilingual support and translation integration
- [ ] Flashcard generator for learners

---

## ğŸ“„ License

This project is open-sourced under the [MIT License](LICENSE).

---

## ğŸ‘¤ Author

**Dijo Daiju**  
ğŸ”— [GitHub](https://github.com/dijo-d) | ğŸ¤— [Hugging Face](https://huggingface.co/dijodaiju)
